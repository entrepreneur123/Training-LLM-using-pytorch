# -*- coding: utf-8 -*-
"""How to train LLM with pytorch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KtP1calGzTvUm5tSdJfASyuIEvwAn_zu

1.   Trl: used to train transformer language models with reinforcement learning.
2. Peft uses the parameter-efficient fine-tuning (PEFT) methods to enable efficient adaptation of the pre-trained model.
3. Torch: a widely-used open-source machine learning library.
4. Datasets: used to assist in downloading and loading many common machine learning datasets.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip -q install trl
# pip -q install peft
# pip -q install torch
# pip -q install datasets
# pip -q install transformers

import torch
from trl import SFTTrainer
from datasets import load_dataset
from peft import LoraConfig,get_peft_model,prepare_model_for_int8_training
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments

train_dataset = load_dataset("tatsu-lab/alpaca",split = "train")
print(train_dataset)

pandas_format = train_dataset.to_pandas()
display(pandas_format.head())

import textwrap

for index in range(3):
  print("..." * 15)
  print("Instruction {}".format(textwrap.fill(pandas_format.iloc[index]["instruction"],width=50)))
  print("Output {}".format(textwrap.fill(pandas_format.iloc[index]["output"],width=50)))
  print("Text {}".format(textwrap.fill(pandas_format.iloc[index]["text"],width=50)))

"""Model Training
some prerequisites


1.   Pre-trained Model: We will use the pre-trained model Salesforce/xgen-7b-8k-base, which is available on Hugging Face. Salesforce trained this series of 7B LLMs named XGen-7B with standard dense attention on up to 8K sequences for up to 1.5T tokens.
2.   Tokenizer: This is needed for tokenization tasks on the training data. The code to load the pre-trained model and tokenizer is as follows:


"""



pretrained_model_name = "Salesforce/xgen-7b-8k-base"
model = AutoModelForCausalLM.from_pretrained(pretrained_model_name, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, trust_remote_code=True)

"""Training Configuration


1.   The training requires some training arguments and configurations.And the tow important configuration objects are defined below,an instance of the training arguments and an instance of LoraConfig model and SFTTrainer model.
2.   Training Arguments are used to define the parameters for model training .

3. In this specific scenario, we start by defining the destination where the trained model will be stored using the output_dir attribute before defining additional hyperparameters, such as the optimization method, the learning rate, the number of epochs, and more.


"""

model_training_args = TrainingArguments(
    output_dir = "xgen-7b-8k-base-fine-tuned",
    pre_device_train_batch_size = 4,
    optim = "adamw_torch",
    logging_steps = 80,
    learning_rate = 2e-4,
    warmup_ratio = 0.1,
    lr_schedular_type = "linear",
    num_train_epochs = 1,
    save_strategy = "epoch"
)

